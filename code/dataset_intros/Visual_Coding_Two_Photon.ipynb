{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092a02df",
   "metadata": {},
   "source": [
    "![Image](resources/banner.jpg)\n",
    "\n",
    "<h1 align=\"center\">Allen Brain Observatory Visual Coding Two-Photon </h1> \n",
    "<h2 align=\"center\"> Day 1, Morning Session. SWDB 2024 </h2> \n",
    "\n",
    "<h3 align=\"center\">Monday, August 19, 2023</h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faf1388",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "The Allen Brain Observatory Visual Coding Two-Photon dataset is a large-scale survey of physiological activity in mouse visual cortex in response to a variety of visual stimuli under passive viewing conditions.  The animals are head-fixed but free to run on a disc.  Single plane two-photon calcium imaging is performed in different areas and layers with transgenically targeted cell lines.  This notebook is a brief introduction to get you started with this data set and lead you to resources for you to explore further.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c8786",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "***What kind of questions can you answer with this dataset?***\n",
    "\n",
    "This dataset contains recordings of activity in response to a variety of natural and artificial visual stimuli.  This makes it suitable for a variety of coding questions.\n",
    "\n",
    "- How are stimuli and features from the external world encoded in neural responses?  \n",
    "- How do the encoding properties differ across areas and layers?  In different cell lines?\n",
    "- Can you build predictive models of response from stimuli?\n",
    "- How are running activity and pupil size related to cortical activity?\n",
    "- How can information about the stimuli and/or the animal's state be extracted from neural activity?  Can you decode stimuli?\n",
    "- Do neurons coordinate their activity?  Do the act in ensembles?  \n",
    "- Is there any spatial aspect to neural information?\n",
    "\n",
    "These are just some of the questions that might be addressed from this type of data.  \n",
    "\n",
    "***Why two-photon calcium imaging?***\n",
    "\n",
    "- You get a relatively large number of cells across an area.\n",
    "- 2D spatial arrangement within a layer.\n",
    "- Chronic recording across days, and thus more measurements or stimuli.\n",
    "\n",
    "***Why NOT two-photon calcium imaging?***\n",
    "\n",
    "- Indirect measure of activity.  One must decide how to extract \"activity\" from the calcium signal, and what that means.\n",
    "- Time scale of calcium is slow; you get relatively poor temporal resolution.\n",
    "- For the indicator and resolution at which these recordings were made, single and low spike count activity is often not observed.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304a5ec",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "**Databook**\n",
    "\n",
    "The databook is a resource for more in-depth information and examples for the Allen Brain Observatory Visual Coding Two-photon dataset.  You can find the pages for this data set here:  https://allenswdb.github.io/physiology/ophys/visual-coding/vc2p-background.html\n",
    "\n",
    "![Image](resources/databook_vc2p.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf63e8",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "***Remember the tools you have!***\n",
    "\n",
    "- Use the databook as a reference; this notebook contains only a small portion of what is in the databook!\n",
    "- Use the help function to find function arguments\n",
    "- Use `dir` to see data and functions in an object\n",
    "- Use tab complete in jupyter \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cccadf",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "Using the Python objects we'll show you below, you can extract information about this dataset such as how many recordings from a given area or Cre line.\n",
    "\n",
    "For each targeted area, layer, and Cre line, each mouse is recorded for three sessions (see more on this below).  There is a datafile for each session that includes (not exhaustive):\n",
    "\n",
    "- Various flourescence traces from different stages of the processing pipeline.\n",
    "- Running activity of the mouse\n",
    "- Pupil size and eye tracking (for some sessions)\n",
    "- Stimulus presentation timing and templates\n",
    "- Max projection images and roi masks for each cell\n",
    "- Extracted event traces from a deconvolution algorithm (in a separate file)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36265f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import platform, os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920f4bd",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "The following cell sets up a path variable so that this notebook will work on the cloud or using data accessed locally, e.g. from your hard drive.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8cf87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set file location based on platform. \n",
    "platstring = platform.platform()\n",
    "if ('Darwin' in platstring) or ('macOS' in platstring):\n",
    "    # macOS \n",
    "    data_root = \"/Volumes/Brain2024/\"\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = \"E:/\"\n",
    "elif ('amzn' in platstring):\n",
    "    # then on Code Ocean\n",
    "    data_root = \"/data/\"\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = \"/media/$USERNAME/Brain2024/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f242601",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "This dataset is accessed via the `allensdk` python package.  It requires instantiating a `BrainObservatoryCache` object that we usually call `boc`.  You'll access all of the data for this dataset using this object.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a4d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "manifest_file = os.path.join(data_root,'allen-brain-observatory/visual-coding-2p/manifest.json')\n",
    "boc = BrainObservatoryCache(manifest_file=manifest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b657dea",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "***Plotting responses and stimulus epochs***\n",
    "\n",
    "To give you an overview of how to access and use this data set, we are going to demonstrate accessing data for a session and plotting traces overlayed with stimulus epochs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25638850",
   "metadata": {},
   "source": [
    "![Image](resources/vc2p.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4895051b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "You can get general information, such as the available areas or stimuli using queries that often start with `get_`.  Use introspection or see the databook for other possibilities.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be01b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boc.get_all_targeted_structures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa56fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "boc.get_all_stimuli()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a104f809",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "The experiments are arranged in *containers*, which are a set of recording sessions that include a complete set of stimuli.  In this dataset, there are three sessions per container.  \n",
    "\n",
    "![Image](resources/VC2p-sessions.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bcc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_containers = boc.get_experiment_containers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072566b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(experiment_containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = boc.get_ophys_experiments(experiment_container_ids=[511510911])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf4472",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d1868",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "Note:  `id` in the experiment containers table is the *container id*.  `id` in the sessions table is the *session id*.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63eb6f4",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "![Image](resources/stim_container.png)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1ae9c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "The `get_ophys_experiment_data` method will instantiate an object that contains the actual data for a single session.  If you do not have the data properly mounted (either on Code Ocean or via your hard drive) you will get a warning that the data is being downloaded here.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939735e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = 508356957\n",
    "session_data = boc.get_ophys_experiment_data(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddce8f3",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "The processed DF/F traces for that session can be returned via the following function.  This returns a tuple containing time stamps and a numpy array of shape (neurons, acquisition frames).  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f82c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "t, dff = session_data.get_dff_traces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1470394",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(t, dff[n])\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('DF/F (arbitrary units)')\n",
    "ax.set_title('DF/F trace for cell index {}'.format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31435446",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "***Task 1***\n",
    "\n",
    "Choose different cell indices and remake the plot above.  Can you find cells with intereseting responses?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7eb24",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "In our analyses, we often use \"extracted events\", which are deconvolved fluorescence traces using an algorithm from Daniela Witten and Sean Jewell.  These are not in the session_data object but are accessed via a function from `boc`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d932e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = boc.get_ophys_experiment_events(ophys_experiment_id=session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(t, events[n])\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('DF/F (arbitrary units)')\n",
    "ax.set_title('DF/F trace for cell index {}'.format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4cab1c",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "Information about when each stimulus type is shown is contained in the `stimulus_epoch_table`.  `start` and `end` denote the *acquisition frame* on which that stimulus epoch began or ended.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_epoch = session_data.get_stimulus_epoch_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be40d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_epoch_table = pd.DataFrame(stim_epoch)\n",
    "stim_epoch_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7a57b",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "We will use the function `ax.axvspan` to shade the temporal window during which a single stimulus epoch occured.  First let's grab the `start` and `end` frames for the epoch during which `natural_movie_one` was shown.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d3279",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "***Task 2***\n",
    "\n",
    "Remake the plot of extracted events vs. time.  Use the function axvspan and the stimulus_epoch_table to shade the region during which natural_movie_one was shown.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = stim_epoch_table[stim_epoch_table.stimulus=='natural_movie_one'].start.iloc[0]\n",
    "end = stim_epoch_table[stim_epoch_table.stimulus=='natural_movie_one'].end.iloc[0]\n",
    "start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0f313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(t, events[n])\n",
    "ax.axvspan(xmin=t[start], xmax=t[end], color='r', alpha=0.1)\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('DF/F (arbitrary units)')\n",
    "ax.set_title('DF/F trace for cell index {}'.format(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9bcf8",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "***Task 3***\n",
    "\n",
    "A.  Remake the previous plot.  Add all of the stimulus epochs, with a unique color for each stimulus type.  (Hint:  define a list of colors beforehand.)\n",
    "\n",
    "B.  Add all of the other traces in the experiment, using a vertical offset so they don't overlap.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['blue','orange','green','red']\n",
    "n = 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "ax.plot(t, events[n])\n",
    "\n",
    "for c,stim_name in enumerate(stim_epoch.stimulus.unique()):\n",
    "    stim = stim_epoch[stim_epoch.stimulus==stim_name]\n",
    "    for j in range(len(stim)):\n",
    "        ax.axvspan(xmin=t[stim.start.iloc[j]], xmax=t[stim.end.iloc[j]], color=colors[c], alpha=0.1)\n",
    "\n",
    "\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('DF/F (arbitrary units)')\n",
    "ax.set_title('DF/F trace for cell index {}'.format(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e64364",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "\n",
    "#here we plot the first 50 neurons in the session\n",
    "for i in range(50):\n",
    "    ax.plot(t, dff[i,:]+(i*2), color='gray')\n",
    "    \n",
    "#here we shade the plot when each stimulus is presented\n",
    "colors = ['blue','orange','green','red']\n",
    "for c,stim_name in enumerate(stim_epoch.stimulus.unique()):\n",
    "    stim = stim_epoch[stim_epoch.stimulus==stim_name]\n",
    "    for j in range(len(stim)):\n",
    "        ax.axvspan(xmin=t[stim.start.iloc[j]], xmax=t[stim.end.iloc[j]], color=colors[c], alpha=0.1)\n",
    "        \n",
    "ax.set_xlabel(\"time (s)\")\n",
    "ax.set_ylabel(\"Extracted Events (arbitrary units)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f68903-086c-4427-a490-f3d3cea9bbc0",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "   \n",
    "***Explore further***\n",
    "\n",
    "- Above we retrieved the targeted structures and the available stimuli using methods like 'boc.get_all_targeted_structures'. Using similar methods, what areas and depths are available?  What other dimensions of the data can be acquired this way?\n",
    "\n",
    "- How can you retrieve a list of only those experiment containers from a particular area or Cre line?\n",
    "\n",
    "- How would you retreive all sessions that included a particular stimulus, say natural scenes?\n",
    "\n",
    "- What other traces are available in the session data?  What do they represent?\n",
    "\n",
    "- How would you retreive the running speed of the animal?  Add a trace of the running speed to the plot above.\n",
    "\n",
    "- What is image #48 in the Natural Scenes stimulus?\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: dropdown\n",
    "Remember to check the [Databook](https://allenswdb.github.io/physiology/ophys/visual-coding/vc2p-background.html)!\n",
    ":::\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f4902",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "***Homework 1***\n",
    "\n",
    "Make a plot similar to the above with shading for the times when an individual frame of natural scenes was presented.  \n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: dropdown\n",
    "The stimulus epoch table shows you when classes of stimuli are on the screen.  There is a similar data structure called the stimulus table that is specific to each data set.  \n",
    ":::\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf2451",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "***Homework 2***\n",
    "\n",
    "Make a plot of the max projection image for a single session.  Compute the average activity across the sessions for each cell and shade the rois in the max projection image according to that activity.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccdad29-f21e-4f88-9de6-d070413c7757",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "***Homework 3***\n",
    "\n",
    "Make a DataFrame of the number of session per Cre line and area.\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: dropdown\n",
    "Remember to check the [Databook](https://allenswdb.github.io/physiology/ophys/visual-coding/vc2p-background.html)!\n",
    ":::\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865e020-8ee8-4928-bcf6-7157c47031b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "***Homework 4***\n",
    "\n",
    "List these Cre lines in order of how many neurons they have per session (on average):\n",
    "    Rbp4-Cre_KL100\n",
    "    Cux2-CreERT2\n",
    "    Rorb-IRES2-Cre\n",
    "    Vip-IRES-Cre\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753aedf",
   "metadata": {},
   "source": [
    "<div style=\"background: #DFF0D8; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "***Homework 5***\n",
    "\n",
    "As described above, the same set of neurons are targeted for each of the three sessions in a container, but not all cells appear in every session (see the graphic below).  Identify the cells that are common are across all three sessions in a container and remake the plot above with just those cells.  \n",
    "\n",
    "\n",
    ":::{admonition} Hint\n",
    ":class: dropdown\n",
    "The indices for the dff traces are specific to each session.  To connect cells across sessions you will need to know what a `cell_specimen_id` is.  Remember to check the [Databook](https://allenswdb.github.io/physiology/ophys/visual-coding/vc2p-background.html)!\n",
    ":::\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97177210",
   "metadata": {},
   "source": [
    "\n",
    "![Image](resources/cell_specimens.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
